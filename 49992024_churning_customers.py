# -*- coding: utf-8 -*-
"""49992024_Churning_Customers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nloBRopwTB8Eq4C0Qp-hsxMaqjERVGrx

# Customer Churning
"""

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/My Drive/AIClass/datasets_13996_18858_WA_Fn-UseC_-Telco-Customer-Churn.csv')

df

df.info()

"""## Imputation and Encoding"""

#Imputing for the numerical values
numerical_imputer = SimpleImputer(strategy='mean')
df_imputed_numerical = pd.DataFrame(numerical_imputer.fit_transform(df.select_dtypes(include='number')), columns=df.select_dtypes(include='number').columns)

# Imputing for Binary values
imputer = SimpleImputer(strategy="most_frequent")
df["Churn"] = df["Churn"].str.lower()
df["Churn"] = imputer.fit_transform(df[["Churn"]]).ravel()


#Imputing for the categorical values
for column in df.select_dtypes(include='object').columns:
    df[column] = df[column].fillna(df[column].mode()[0])

#Using LabelEncoder
label_encoder = LabelEncoder()

#Encoding for Binary values
df['Churn'] = df['Churn'].map({'yes': 1, 'no': 0})


#Encoding the categorical values
for column in df.select_dtypes(include='object').columns:
    df[column] = label_encoder.fit_transform(df[column])

df_final = pd.concat([df_imputed_numerical, df.select_dtypes(include='object')], axis=1)
df.info()

"""## EDA

Training the model using Feature importance
"""



y=df['Churn']
X= df.drop(columns=['Churn'])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Scaling the dataset
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
# Save the feature names along with the scaler
feature_names = X_train.columns.tolist()
with open("my_scaler.pkl", 'wb') as sc:
    pickle.dump((scaler, feature_names), sc)

# RandomForestRegressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
print("Feature Ranking:")
for i in range(X.shape[1]):
  print(f"{i + 1}.Feature {X.columns[indices[i]]}({importances[indices[i]]})")

"""Using the feature importance it has been decided that he top 10 features will be taken into consideration

Take into consideration that 0 means 'No' and 1 means 'Yes' as previously shown within the code
"""

target_instance = df["Churn"].value_counts().to_frame()
target_instance = target_instance.reset_index()
target_instance = target_instance.rename(columns={'index': 'Category'})
fig = px.pie(target_instance, values='Churn', names='Category', color_discrete_sequence=["green", "red"],
             title='Distribution of Churn')
fig.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='MonthlyCharges', hue='Churn', bins=20, kde=True, palette='viridis')
plt.xlabel('Monthly Charges')
plt.ylabel('Frequency')
plt.title('Distribution of Monthly Charges for Churn and No Churn')
plt.show()

# Group the data by the categorical factor and calculate the mean of the value column
grouped_df = df.groupby('Churn')['Contract'].mean()

# Plot the bar chart
plt.bar(grouped_df.index, grouped_df.values)
plt.xlabel('Churn')
plt.ylabel('Contract')
plt.title('Churn against Contract')
plt.show()

grouped_df = df.groupby('Churn')['TotalCharges'].mean()

# Plot the bar chart
plt.bar(grouped_df.index, grouped_df.values)
plt.xlabel('Churn')
plt.ylabel('TotalCharges')
plt.title('Churn against TotalCharges')
plt.show()

"""## Training the Model"""

import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from tensorflow.compat.v1.keras.backend import set_session
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth=True

set_session(tf.compat.v1.Session(config=config))

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import roc_auc_score

X_train.shape

input = Input(shape=(X_train.shape[1],))

# Add hidden layers
hidden_layer1 = Dense(units=64, activation='relu')(input)
hidden_layer2 = Dense(units=32, activation='relu')(hidden_layer1)
hidden_layer3 = Dense(units=16, activation='relu')(hidden_layer1)
hidden_layer4 = Dense(units=8, activation='relu')(hidden_layer1)

# Assuming hidden_layer4 is the output of the previous layer
output_layer = Dense(units=1, activation='sigmoid')(hidden_layer4)

# Creating the model by specifying inputs and outputs
model = Model(inputs=input, outputs=output_layer)

# Compile the model
model.compile(
    loss='binary_crossentropy',
    optimizer=Adam(learning_rate=0.0001),
    metrics=['accuracy']
)

model.fit(X_train, y_train, epochs=100, batch_size=32)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print('Loss:', loss)
print('Accuracy:', accuracy)
# Calculate the AUC score
y_pred = model.predict(X_test)
auc_score = roc_auc_score(y_test, y_pred)
print('AUC score:', auc_score)

"""Training with GridSearch"""

from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV

#Optimizing the model with Gridsearch
# Define the parameter grid
param_grid = {
    'hidden_layer_sizes': [(150,), (110, 40), (110, 40, 30)],
    'activation': ['relu', 'tanh'],
    'solver': ['adam'],
    'learning_rate_init': [0.001, 0.01, 0.1],
    'max_iter': [200, 350, 500]
}

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=MLPClassifier(), param_grid=param_grid, cv=5)

# Fit the model to the training data
grid_search.fit(X_train, y_train)

# Print the best parameters
print('Best parameters:')
print(grid_search.best_params_)

# Evaluate the model on the test data
y_pred = grid_search.predict(X_test)
accuracy = grid_search.score(X_test, y_test)
print('Accuracy:', accuracy)
auc_score = roc_auc_score(y_test, y_pred)
print('AUC score afer optimizatio:', auc_score)

# Saving the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
best_model.fit(X_train, y_train)
best_model_y_pred = best_model.predict(X_test)

# Calculate AUC for the best model
best_model_auc_score = roc_auc_score(y_test, best_model_y_pred)
print('Best Model AUC Score:', best_model_auc_score)

conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Churn', 'Churn'], yticklabels=['Not Churn', 'Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Display classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Save the Keras model
model.save("keras_model.h5")

pickle.dump(best_model, open('mlp_model.pkl','wb'))

with open('mlp_model.pkl', 'rb') as file:
    mlp_model = pickle.load(file)

test = mlp_model.predict(X_test)